# Implementation Plan: Kết nối Database & Thiết kế Data Pipeline

**Branch**: `002-connect-db-and-design-data-pipeline` | **Date**: 2026-02-10 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/002-connect-db-and-design-data-pipeline/spec.md`

## Summary

Extend the Legal Chatbot with:
1. **Supabase integration** (PostgreSQL + pgvector + Storage) as cloud database with SQLite fallback
2. **Automated data pipeline** to crawl, parse, index, and embed Vietnamese legal documents by category
3. **Test case**: Crawl Luật Đất đai 2024 + related nghị định (~500+ articles) with semantic search
4. **Audit trail** for research results and generated contracts to enable verification

Key research findings that changed the spec:
- Embedding model: `bkai-foundation-models/vietnamese-bi-encoder` (768d, legal-trained) instead of `paraphrase-multilingual-MiniLM-L12-v2` (384d)
- Vector index: HNSW (self-updating) instead of IVFFlat
- Vector search: Must use PostgreSQL RPC functions (PostgREST doesn't support pgvector operators)
- Text normalization: NFC for embeddings (NOT NFD as existing code uses)
- Cloudflare active on thuvienphapluat.vn: Requires Playwright + stealth plugin, 3-5s rate limiting

## Technical Context

**Language/Version**: Python 3.11+
**Primary Dependencies**: supabase-py, sentence-transformers, Playwright + stealth, APScheduler
**Storage**: Supabase PostgreSQL + pgvector (production) / SQLite (local fallback)
**Testing**: pytest, pytest-asyncio
**Target Platform**: CLI (Windows/Linux/Mac)
**Project Type**: Single Python package (extending existing)
**Performance Goals**: Pipeline crawl <5 min per category, semantic search <2s, chat response <5s
**Constraints**: Supabase free tier (500MB DB, 1GB Storage), ~1.6 GB RAM for embedding model
**Scale/Scope**: Initial: 6 land law documents (~500 articles). Target: 50+ documents across categories

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

- [x] **Specification-First**: Spec.md complete with use cases, data model, CLI commands, phases
- [x] **Test-First**: Test strategy defined (unit + integration + acceptance tests)
- [x] **Code Quality**: Black + isort + mypy (same as 001)
- [x] **UX Consistency**: CLI commands documented (pipeline, db, audit)
- [x] **Performance**: Pipeline <5 min, search <2s, chat <5s defined
- [x] **Observability**: Pipeline run logging via `pipeline_runs` table + Python logging
- [x] **Issue Tracking**: On branch `002-connect-db-and-design-data-pipeline`

**Complexity Violations**: None identified

## Previous Work

**Feature 001 (planning)**: Built foundation — CLI, crawler (demo), indexer, chat (RAG), PDF generator, SQLite + ChromaDB.

Relevant overlap:
- `db/sqlite.py`: Existing SQLite operations → will be preserved as fallback via abstract interface
- `db/chroma.py`: Keyword-based search → will be superseded by pgvector but kept as fallback
- `services/crawler.py`: Demo crawler with sample URLs → will be extended with category-based crawling + Cloudflare bypass
- `services/indexer.py`: HTML parsing + basic indexing → will be extended with embedding generation
- `services/chat.py`: RAG pipeline → will be updated to query Supabase + save audit
- `services/research.py`: Real-time crawling → will use indexed data instead
- `models/document.py`: LegalDocument, Article models → will be extended with new fields
- `utils/vietnamese.py`: NFD normalization → NFD kept for keyword matching, new NFC function for embeddings

## Project Structure

### Documentation (this feature)

```text
specs/002-connect-db-and-design-data-pipeline/
├── plan.md              # This file
├── research.md          # Phase 0 research findings
├── data-model.md        # Supabase schema + Pydantic models
├── quickstart.md        # Setup guide for Supabase + pipeline
├── contracts/           # Module interface contracts
│   ├── supabase-client.md
│   ├── pipeline.md
│   ├── embedding.md
│   └── audit.md
└── tasks.md             # Phase 2 output (generated by /specledger.tasks)
```

### Source Code (repository root)

```text
legal_chatbot/
├── db/
│   ├── sqlite.py          # Kept: local/offline fallback
│   ├── chroma.py          # Kept: keyword search fallback
│   ├── supabase.py        # NEW: Supabase client + CRUD + RPC
│   └── base.py            # NEW: Abstract DB interface (strategy pattern)
├── services/
│   ├── crawler.py         # EXTEND: category crawl, Cloudflare bypass, change detection
│   ├── indexer.py         # EXTEND: embedding generation, Supabase pgvector storage
│   ├── pipeline.py        # NEW: Pipeline orchestrator (discovery → crawl → index → validate)
│   ├── embedding.py       # NEW: Embedding service (NFC normalize, batch encode, store)
│   ├── scheduler.py       # NEW: Scheduled pipeline runs (APScheduler)
│   ├── audit.py           # NEW: Audit trail service
│   ├── chat.py            # UPDATE: query Supabase, save research audit
│   ├── research.py        # UPDATE: use indexed data, save audit
│   └── generator.py       # UPDATE: save contract audit
├── models/
│   ├── document.py        # EXTEND: category, expiry_date, relations, content_hash
│   ├── pipeline.py        # NEW: PipelineRun, CrawlResult, CategoryConfig
│   └── audit.py           # NEW: ResearchAudit, ContractAudit
├── cli/
│   └── main.py            # EXTEND: pipeline, db, audit command groups
└── utils/
    ├── config.py          # EXTEND: Supabase settings, pipeline settings
    └── vietnamese.py      # EXTEND: normalize_for_embedding() (NFC)

tests/
├── unit/
│   ├── test_supabase.py       # NEW
│   ├── test_pipeline.py       # NEW
│   ├── test_embedding.py      # NEW
│   └── test_audit.py          # NEW
├── integration/
│   ├── test_pipeline_e2e.py   # NEW
│   ├── test_supabase_crud.py  # NEW
│   └── test_audit.py          # NEW
└── contract/
    ├── test_supabase_contract.py  # NEW
    ├── test_pipeline_contract.py  # NEW
    └── test_embedding_contract.py # NEW
```

**Structure Decision**: Extend existing single Python package. New modules added alongside existing ones. Strategy pattern (`db/base.py`) enables switching between SQLite and Supabase.

## Complexity Tracking

> No violations identified.

## Implementation Phases

### Phase 1: Supabase Foundation
- Create Supabase project and enable pgvector extension
- Implement `db/base.py` — abstract interface (`DatabaseInterface` ABC)
- Implement `db/supabase.py` — Supabase client (singleton, CRUD, RPC, Storage)
- Update `utils/config.py` — add `DB_MODE`, `SUPABASE_URL`, `SUPABASE_KEY`, pipeline settings
- Implement dual-mode switching: `DB_MODE=supabase` vs `DB_MODE=sqlite`
- Create SQL migration for all tables + `match_articles` RPC function
- CLI command: `python -m legal_chatbot db migrate`

### Phase 2: Embedding Service
- Implement `services/embedding.py` — NFC normalize, single/batch encode, store
- Add `normalize_for_embedding()` to `utils/vietnamese.py`
- Implement long article splitting (by Khoản when >256 tokens)
- Unit tests for embedding generation + normalization

### Phase 3: Data Pipeline Core
- Implement `services/pipeline.py` — orchestrator (4 phases: discover → crawl → index → validate)
- Implement `models/pipeline.py` — PipelineRun, CrawlResult, CategoryConfig
- Extend `services/crawler.py` — category crawl, Playwright + stealth, content hash
- Extend `services/indexer.py` — call embedding service, store in Supabase
- CLI command: `python -m legal_chatbot pipeline crawl --category dat-dai`

### Phase 4: Test Case — Crawl Đất đai
- Configure category "đất đai" with verified URLs from research
- Crawl Luật Đất đai 2024 + NĐ 102/2024 + NĐ 101/2024 (3 documents minimum)
- Verify parsed data quality (article count, structure)
- Test semantic search: "Điều kiện chuyển nhượng quyền sử dụng đất?"
- Validate results cite Luật Đất đai 2024 articles

### Phase 5: Audit Trail
- Implement `services/audit.py` + `models/audit.py`
- Update `services/chat.py` — save ResearchAudit after each response
- Update `services/generator.py` — save ContractAudit after each generation
- CLI commands: `audit list`, `audit verify <id>`

### Phase 6: Integration & Polish
- Update CLI command groups (pipeline, db, audit)
- Implement `services/scheduler.py` (optional — manual trigger first)
- End-to-end testing: crawl → index → chat → audit
- Update requirements.txt with new dependencies

## Dependencies

```
# New
supabase>=2.0.0
sentence-transformers>=2.2.0
playwright-stealth>=1.0.0
apscheduler>=3.10.0
datasets>=2.0.0               # Optional: HuggingFace bootstrap

# Existing (unchanged)
groq>=0.4.0
typer>=0.9.0
rich>=13.0.0
reportlab>=4.0.0
playwright>=1.40.0
beautifulsoup4>=4.12.0
aiohttp>=3.9.0
pydantic>=2.0.0
python-dotenv>=1.0.0
pytest>=7.0.0
pytest-asyncio>=0.21.0
```

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| Cloudflare blocks crawler | Playwright + stealth + 3-5s delay + Firefox UA |
| 256-token embedding limit | Split long articles by Khoản with Điều header |
| Supabase free tier limits | Compress raw HTML, prioritize essential docs |
| Model download (1.1 GB) | One-time download, cache in `~/.cache/` |
| thuvienphapluat.vn structure changes | HuggingFace dataset as fallback bootstrap |
| NFD/NFC confusion | Separate functions: `normalize_vietnamese()` (NFD) vs `normalize_for_embedding()` (NFC) |

## Success Criteria

1. **Supabase**: Connect, CRUD, vector search via RPC function working
2. **Pipeline**: Crawl 3+ land law documents, parse 200+ articles, generate embeddings
3. **Search**: Semantic search returns relevant articles with >0.5 similarity
4. **Audit**: Research and contract operations logged with law versions
5. **Dual mode**: Switch between Supabase and SQLite via `DB_MODE` env var

## Next Steps

1. Run `/specledger.tasks` to generate detailed task breakdown
2. Setup Supabase project and get credentials
3. Begin Phase 1 implementation
